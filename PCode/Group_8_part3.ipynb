{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddbd1aa",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "### Problem Statement\n",
    "Train a good penalised logistic regression model on the competition data. The\n",
    "only type of model you should use for this part, is the H2OGeneralizedLinearEstimator\n",
    "model.\n",
    "\n",
    "You should:\n",
    "\n",
    "- Deal appropriately with missings (for all numeric variables, -99 means missing).\n",
    "- Deal with numerics - i.e. for at least some try linear splines (or another method of your choice to deal with non-linear effects).\n",
    "- Deal with hccvs (eg using the feature encoding library that we looked at in lecture) (You do not need to deal with low cardinality categorical features since H2O will one-hot them for you).\n",
    "- Try out some interactions.\n",
    "- Try out some other features (eg division of numerics).\n",
    "\n",
    "*Disclaimer: Some preprocessing processes were using the same code as of Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd055328",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05106764",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eaac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "import h2o\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9497e94",
   "metadata": {},
   "source": [
    "### Set directories and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49621101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/smm284-aml/assignment/jimmy\n"
     ]
    }
   ],
   "source": [
    "# set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = os.path.join('..', 'input')\n",
    "dirPData   = os.path.join('..', 'PData')\n",
    "dirPOutput = os.path.join('..', 'POutput')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ad674",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4a6f88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data: 250k train data\n",
    "f_name = '01_df_250k.pickle'\n",
    "f_path = os.path.join(dirPData, f_name)\n",
    "PData = pickle.load(open(f_path, \"rb\"))\n",
    "\n",
    "# separate data into train/test set\n",
    "train_set = PData['df_train']\n",
    "test_set = PData['df_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd050736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load variable metadata \n",
    "f_name = '01_vars.pickle'\n",
    "f_path = os.path.join(dirPData, f_name)\n",
    "var_meta = pickle.load(open(f_path, \"rb\"))\n",
    "\n",
    "# extract lists from metadata\n",
    "var_idx_num = var_meta['vars_ind_numeric']\n",
    "var_idx_cat = var_meta['vars_ind_categorical']\n",
    "var_idx_hccv = var_meta['vars_ind_hccv']\n",
    "var_idx_id = var_meta['vars_notToUse']\n",
    "var_idx_response = var_meta['var_dep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf530df2",
   "metadata": {},
   "source": [
    "### Clean missing values\n",
    "#### - Deal appropriately with missings (for all numeric variables, -99 means missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98ed2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace -99 with np.nan\n",
    "train_set = train_set.replace(-99, np.nan)\n",
    "test_set = test_set.replace(-99, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f2056b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0, 102984,\n",
       "        23788,  23788,  23788,   1852,   2090,  23898,  24321,  11673,\n",
       "        32806,  33764, 197803,   8895,   6151,  55544,  57869,  57136,\n",
       "            2,      0,      0,      0,      0,      0,      0,   1092,\n",
       "            0,      0,      0,      0,      0,      0,      0,      1,\n",
       "            1,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of nulls in each column -- train_set\n",
    "np.array(train_set.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26d473a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0, 283601,\n",
       "            0,      0,      0,      0,      0,      2,      0,      1,\n",
       "            0,      0,      0,      0,      0,      1,      0,      9,\n",
       "          226,  10870,      9,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,     17, 131970,  26261,\n",
       "        26261,  26261,   3429,   3504,  26298,  26356,  18076,   2261,\n",
       "         2356,   2756,  10130,   6363,  18816,  22520,  20432,      2,\n",
       "            0,      0,      0,      0,      0,      0,   3560,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of nulls in each column -- test_set\n",
    "np.array(test_set.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c7b828",
   "metadata": {},
   "source": [
    "Strategy:\n",
    "- remove the feature if the number of null values in the `test_set` exceed 250\n",
    "- drop rows that have leftover null values in the `train_set`\n",
    "- for leftover null values in the `test_set` impute with `mean` for numerical features and `most frequent` for categorical features\n",
    "\n",
    "Through data exploration, we decided to remove features that have over 250 null values in the test set. That is because imputation can introduce biases to the data as it is impossible to perfectly predict the true values that are missing. Still, it is not possible to remove observations from the test set concerning the competition. We want to preserve as much information as possible, and thus, for features with fewer than 250 null values (less than 0.1% of the data), imputation for mean / most frequent for numeric / categorical features is incorporated as imputing the small number of observations with these values does not have a significant impact on the overall distribution.\n",
    "\n",
    "For the train set, after removing features with over 250 null values in the test set, very small numbers of null values are observed (5 in total). We decided to remove those observations, and thus we only lose 3 observations in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093a34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of null values in the test set exceed 250\n",
    "to_remove = test_set.columns[np.array(test_set.isnull().sum(axis = 0) > 250)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cca421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the variables\n",
    "train_set = train_set.drop(columns=to_remove)\n",
    "test_set = test_set.drop(columns=to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecacbfed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display number of leftover missing values in `train_set`\n",
    "np.array(train_set.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "945e9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows that have leftover null values in the `train_set`\n",
    "train_set = train_set.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e447b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   2,   0,   1,   0,   0,   0,\n",
       "         0,   0,   1,   0,   9, 226,   9,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  17,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display number of leftover missing values in `test_set`\n",
    "np.array(test_set.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc9306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['c09', 'e03', 'e24', 'e17', 'e18', 'e20', 'f10', 'e02'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get columns in the test_set with missing values\n",
    "impute_col = test_set.columns[np.array(test_set.isnull().sum(axis = 0) > 0)]\n",
    "impute_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca070449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c09</th>\n",
       "      <th>e03</th>\n",
       "      <th>e24</th>\n",
       "      <th>e17</th>\n",
       "      <th>e18</th>\n",
       "      <th>e20</th>\n",
       "      <th>f10</th>\n",
       "      <th>e02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>920BD</td>\n",
       "      <td>11250</td>\n",
       "      <td>A4893</td>\n",
       "      <td>CKG</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>F33BC</td>\n",
       "      <td>5614E</td>\n",
       "      <td>D53C5</td>\n",
       "      <td>AIR</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>F33BC</td>\n",
       "      <td>5614E</td>\n",
       "      <td>D53C5</td>\n",
       "      <td>AEL</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>861C8</td>\n",
       "      <td>9076B</td>\n",
       "      <td>A4893</td>\n",
       "      <td>CUO</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>861C8</td>\n",
       "      <td>9076B</td>\n",
       "      <td>A4893</td>\n",
       "      <td>AWV</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  c09 e03 e24    e17    e18    e20  f10   e02\n",
       "0   B   A   A  920BD  11250  A4893  CKG   6.0\n",
       "1   B   A   A  F33BC  5614E  D53C5  AIR  15.0\n",
       "2   B   A   A  F33BC  5614E  D53C5  AEL  15.0\n",
       "3   A   A   A  861C8  9076B  A4893  CUO  27.0\n",
       "4   A   A   A  861C8  9076B  A4893  AWV  27.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display data for columns that are going to impute\n",
    "to_impute = test_set[impute_col]\n",
    "to_impute.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3907fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column indices for numerical and categorical features that need imputation\n",
    "num_col = to_impute.select_dtypes(include='number').columns\n",
    "cat_col = to_impute.select_dtypes(include='category').columns\n",
    "# create imputation pipeline\n",
    "# -- parameters = List of (name, transformer, columns)\n",
    "pipeline = ColumnTransformer([\n",
    "    ('cat_impute', SimpleImputer(strategy='most_frequent'), cat_col),\n",
    "    ('num_impute', SimpleImputer(strategy='mean'), num_col)\n",
    "])\n",
    "# impute missing values\n",
    "imputed_test = pipeline.fit_transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "824a19ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign values back to the `test_set`\n",
    "test_set[impute_col] = imputed_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51f964a",
   "metadata": {},
   "source": [
    "Check again the number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e9351df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in `train_set`: 0\n",
      "Number of missing values in `test_set`: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of missing values in `train_set`: {np.sum(np.sum(train_set.isna()))}')\n",
    "print(f'Number of missing values in `test_set`: {np.sum(np.sum(test_set.isna()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190fd37",
   "metadata": {},
   "source": [
    "As some features were removed, we adjust the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49eaecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_idx_num = list(set(var_idx_num) - set(to_remove))\n",
    "var_idx_cat = list(set(var_idx_cat) - set(to_remove))\n",
    "var_idx_hccv = list(set(var_idx_hccv) - set(to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed403d",
   "metadata": {},
   "source": [
    "There is also a feature with only 1 unique value: `'e16'`, we removed it from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3b5ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e16'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column with only 1 unique value\n",
    "train_set.columns[(train_set.nunique() == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22501c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from the dataframes\n",
    "train_set = train_set.drop(columns='e16')\n",
    "test_set = test_set.drop(columns='e16')\n",
    "\n",
    "# update metadata\n",
    "var_idx_num.remove('e16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8987a8",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e0136",
   "metadata": {},
   "source": [
    "#### - Deal with numerics - i.e. for at least some try linear splines (or another method of your choice to deal with non-linear effects).\n",
    "  - Apply linear splines to numeric features with more than 20 unique values\n",
    "  - Spline using the percentiles from the train set and applies the same transformation to the test set. \n",
    "  \n",
    "Similar logic as other preprocessing (e.g. standardisation), the transformation is fitted using the training set and applied to the test. The test set should be kept as unseen (i.e., not knowing the distribution during the training process), and thus, the linear spline should be applied base on the distribution of the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f026cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract numeric features from train/test sets\n",
    "df_train_num = train_set[var_idx_num]\n",
    "df_test_num = test_set[var_idx_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2f006b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_to_spline:\n",
      " ['e08', 'e05', 'e04', 'e15', 'e02', 'f02']\n"
     ]
    }
   ],
   "source": [
    "# select only features with over 20 unique values to spline\n",
    "var_to_spline = list(df_train_num.columns[df_train_num.nunique().values > 20])\n",
    "print('var_to_spline:\\n',\n",
    "      var_to_spline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a617118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a spline function that transform both train/test set\n",
    "def fn_spline_train_test(var, x_train, x_test, n_spline):\n",
    "    # define percentile step size\n",
    "    step = 100 // n_spline\n",
    "    # get the percentiles from the train set\n",
    "    ptiles = np.percentile(x_train, np.arange(step, 100+step, step))\n",
    "    # initialise dataframes\n",
    "    train_ptiles = pd.DataFrame({var: x_train})\n",
    "    test_ptiles = pd.DataFrame({var: x_test})\n",
    "    # spline the variable\n",
    "    for idx, ptile in enumerate(ptiles):\n",
    "        train_ptiles[f'{var}_{str(idx)}'] = np.maximum(0, x_train - ptiles[idx])\n",
    "        test_ptiles[f'{var}_{str(idx)}'] = np.maximum(0, x_test - ptiles[idx])\n",
    "    return [train_ptiles, test_ptiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc3a1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applies linear spline to both train/test sets\n",
    "for var in var_to_spline:\n",
    "    # generate a dataframe for spline variables\n",
    "    train_ptiles, test_ptiles = fn_spline_train_test(var=var, \n",
    "                                                     x_train=df_train_num[var],\n",
    "                                                     x_test=df_test_num[var],\n",
    "                                                     n_spline=5)\n",
    "    # drop the variable that were transformed\n",
    "    df_train_num = df_train_num.drop(columns=[var])\n",
    "    df_test_num = df_test_num.drop(columns=[var])\n",
    "    # concat the spline variables\n",
    "    df_train_num = pd.concat([df_train_num, train_ptiles], axis=1, sort=False)\n",
    "    df_test_num = pd.concat([df_test_num, test_ptiles], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b7dec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f26</th>\n",
       "      <th>f25</th>\n",
       "      <th>e06</th>\n",
       "      <th>f01</th>\n",
       "      <th>f08</th>\n",
       "      <th>f22</th>\n",
       "      <th>e07</th>\n",
       "      <th>f15</th>\n",
       "      <th>f13</th>\n",
       "      <th>e09</th>\n",
       "      <th>...</th>\n",
       "      <th>e02_1</th>\n",
       "      <th>e02_2</th>\n",
       "      <th>e02_3</th>\n",
       "      <th>e02_4</th>\n",
       "      <th>f02</th>\n",
       "      <th>f02_0</th>\n",
       "      <th>f02_1</th>\n",
       "      <th>f02_2</th>\n",
       "      <th>f02_3</th>\n",
       "      <th>f02_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64</td>\n",
       "      <td>45.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74</td>\n",
       "      <td>55.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78</td>\n",
       "      <td>59.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   f26  f25  e06  f01  f08  f22  e07  f15  f13  e09  ...  e02_1  e02_2  e02_3  \\\n",
       "0    0    0   20   33   51    0    0    0   10   43  ...    0.0    0.0    0.0   \n",
       "1    0    0   76   13    0    0    0    0    9   18  ...    0.0    0.0    0.0   \n",
       "2    0    0   76   13   51    1    0    0    9   18  ...    0.0    0.0    0.0   \n",
       "3    0    0   76   13   51    0    0    0    9   18  ...    0.0    0.0    0.0   \n",
       "4    0    0   76   13   51    0    0    0    8   18  ...    0.0    0.0    0.0   \n",
       "\n",
       "   e02_4  f02  f02_0  f02_1  f02_2  f02_3  f02_4  \n",
       "0    0.0   43   24.0    3.0    0.0    0.0    0.0  \n",
       "1    0.0   31   12.0    0.0    0.0    0.0    0.0  \n",
       "2    0.0   64   45.0   24.0    6.0    0.0    0.0  \n",
       "3    0.0   74   55.0   34.0   16.0    0.0    0.0  \n",
       "4    0.0   78   59.0   38.0   20.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display data\n",
    "df_test_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afb6051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata for numeric features after spline\n",
    "var_idx_spline = df_train_num.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31afbe8",
   "metadata": {},
   "source": [
    "#### - Deal with hccvs (eg using the feature encoding library that we looked at in lecture) (You do not need to deal with low cardinality categorical features since H2O will one-hot them for you).\n",
    "For this part, we used Target Encoder to get the mean of the response variable within each class. \n",
    "\n",
    "*It performs better than the other encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c9db901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hccv features from train/test sets\n",
    "df_train_hccv = train_set[var_idx_hccv]\n",
    "df_test_hccv = test_set[var_idx_hccv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c16382ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# encode hccv with target encoder\n",
    "enc = TargetEncoder()\n",
    "# apply target encoder\n",
    "df_train_hccv = enc.fit_transform(df_train_hccv, train_set[var_idx_response])\n",
    "df_test_hccv = enc.transform(df_test_hccv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be6b9b",
   "metadata": {},
   "source": [
    "#### - Try out some interactions.\n",
    "We decided to try applying interaction among the 6 top categorical variables based on the feature importances performed in part 2. (We used 6 as there is a gap between the 6th and 7th features.\n",
    "\n",
    "The variables are as follows: `['e21', 'b04', 'f09', 'e20', 'e11', 'a03']`\n",
    "\n",
    "#### - Try out some other features (e.g. division of numerics).\n",
    "We decided to try the division of numeric features. Considering the features we used to apply linear spline, they are deemed higher important features with larger varieties (i.e., unique values > 20). \n",
    "\n",
    "The variables are as follows: `['e04', 'e05', 'e02', 'e08', 'e15', 'f02']`\n",
    "\n",
    "From the below table, we noticed that many `e04, e05, etc` have quite close distribution (mean and std). As well as `'f02', 'f01'`;\n",
    "Therefore, we decided to perform division of these pairs:\n",
    "`[('f01', 'f02'), ('e04', 'e05'), ('e02', 'e15')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51d524ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e09</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>50.191974</td>\n",
       "      <td>27.948693</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e08</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>50.031268</td>\n",
       "      <td>28.215267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e04</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.824678</td>\n",
       "      <td>28.819172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f02</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.641864</td>\n",
       "      <td>28.872189</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f01</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.588539</td>\n",
       "      <td>28.139675</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e02</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.431249</td>\n",
       "      <td>28.883891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e15</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.427393</td>\n",
       "      <td>28.604578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e06</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.387841</td>\n",
       "      <td>27.694698</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f06</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.367396</td>\n",
       "      <td>20.585375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f08</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>49.187510</td>\n",
       "      <td>9.329284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e05</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>48.982576</td>\n",
       "      <td>28.819578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f11</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>9.619083</td>\n",
       "      <td>4.080734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f13</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>8.035104</td>\n",
       "      <td>2.613201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f28</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>4.865150</td>\n",
       "      <td>0.740751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e23</th>\n",
       "      <td>249997.0</td>\n",
       "      <td>2.477346</td>\n",
       "      <td>0.943628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count       mean        std   min   25%   50%   75%   max\n",
       "e09  249997.0  50.191974  27.948693  18.0  18.0  55.0  76.0  99.0\n",
       "e08  249997.0  50.031268  28.215267   0.0  16.0  53.0  75.0  99.0\n",
       "e04  249997.0  49.824678  28.819172   1.0  24.0  49.0  76.0  99.0\n",
       "f02  249997.0  49.641864  28.872189   2.0  25.0  49.0  74.0  99.0\n",
       "f01  249997.0  49.588539  28.139675  13.0  13.0  51.0  75.0  99.0\n",
       "e02  249997.0  49.431249  28.883891   0.0  23.0  50.0  75.0  99.0\n",
       "e15  249997.0  49.427393  28.604578   0.0  22.0  51.0  74.0  99.0\n",
       "e06  249997.0  49.387841  27.694698  20.0  20.0  49.0  76.0  99.0\n",
       "f06  249997.0  49.367396  20.585375   0.0  60.0  60.0  60.0  60.0\n",
       "f08  249997.0  49.187510   9.329284   0.0  51.0  51.0  51.0  51.0\n",
       "e05  249997.0  48.982576  28.819578   0.0  26.0  49.0  74.0  99.0\n",
       "f11  249997.0   9.619083   4.080734   1.0   7.0  10.0  13.0  20.0\n",
       "f13  249997.0   8.035104   2.613201   1.0   6.0   9.0   9.0  15.0\n",
       "f28  249997.0   4.865150   0.740751   0.0   5.0   5.0   5.0   5.0\n",
       "e23  249997.0   2.477346   0.943628   1.0   2.0   2.0   3.0   5.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display distribution of numeric features\n",
    "train_set[var_idx_num].describe().T.sort_values(by='mean', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5525a4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division features: ['f01_f02', 'e04_e05', 'e02_e15']\n"
     ]
    }
   ],
   "source": [
    "# division features\n",
    "# initialise dataframes\n",
    "df_train_div = pd.DataFrame()\n",
    "df_test_div = pd.DataFrame()\n",
    "# specify pairs\n",
    "div_pairs = [('f01', 'f02'), ('e04', 'e05'), ('e02', 'e15')]\n",
    "# iterate\n",
    "for pair in div_pairs:\n",
    "    # division feature: added by '1e-6' to avoid `0/0` case\n",
    "    div_train = np.divide((train_set[pair[0]]+1e-6), (train_set[pair[1]]+1e-6))\n",
    "    div_test = np.divide((test_set[pair[0]]+1e-6), (test_set[pair[1]]+1e-6))\n",
    "    # name the series\n",
    "    div_train = div_train.rename(f'{pair[0]}_{pair[1]}')\n",
    "    div_test = div_test.rename(f'{pair[0]}_{pair[1]}')\n",
    "    # append data to the initilised dataframes\n",
    "    df_train_div = pd.concat([df_train_div, pd.DataFrame(div_train)], axis=1, sort=False)\n",
    "    df_test_div = pd.concat([df_test_div, pd.DataFrame(div_test)], axis=1, sort=False)\n",
    "\n",
    "# create a metadata for these features\n",
    "var_idx_div =  df_train_div.columns.tolist()\n",
    "print(f'Division features: {var_idx_div}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d119a",
   "metadata": {},
   "source": [
    "Interaction among categorical pairs causes error in our training which we suspected it was due to some interactions in the val/test sets not exists in the train/design sets. Therefore, for this assignment, we try the interaction term between categorical variables (`['e21', 'b04', 'f09', 'e20', 'e11', 'a03']`) and non-categorical variables that attained the largest normalized coefficients, including target encoded hccv (`'f10'`) and a numeric variable (`'f02'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c473bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the interaction pairs\n",
    "for_pairs = ['e21', 'b04', 'f09', 'e20', 'e11', 'a03']\n",
    "# create interaction pairs\n",
    "interaction_pairs = []\n",
    "for i in range(len(for_pairs)):\n",
    "    for j in range(len(for_pairs)):\n",
    "        if i < j:\n",
    "            interaction_pairs.append(tuple([for_pairs[i], for_pairs[j]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065df54",
   "metadata": {},
   "source": [
    "The below case is not used in our final prediction (This one is for Case 5; see information about Cases below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b1d6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specify the interaction pairs\n",
    "# for_pairs_cat = ['e21', 'b04']\n",
    "# for_pairs_non_cat = ['f10', 'f02']\n",
    "# # create interaction pairs\n",
    "# interaction_pairs_addition = []\n",
    "# for cat in for_pairs_cat:\n",
    "#     for non_cat in for_pairs_non_cat:\n",
    "#         interaction_pairs_addition.append(tuple([cat, non_cat]))\n",
    "# # add additional interaction pairs to the existing set\n",
    "# interaction_pairs.extend(interaction_pairs_addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eae98bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction pairs: [('e21', 'b04'), ('e21', 'f09'), ('e21', 'e20'), ('e21', 'e11'), ('e21', 'a03'), ('b04', 'f09'), ('b04', 'e20'), ('b04', 'e11'), ('b04', 'a03'), ('f09', 'e20'), ('f09', 'e11'), ('f09', 'a03'), ('e20', 'e11'), ('e20', 'a03'), ('e11', 'a03')]\n"
     ]
    }
   ],
   "source": [
    "print(f'Interaction pairs: {interaction_pairs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3691b40",
   "metadata": {},
   "source": [
    "#### Concatenate all the preprocessed dataframes\n",
    "Prepare dataframes for model training: train/val/test/design sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a51fb71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude hccv from cat variables metadata\n",
    "var_idx_cat_not_hccv = list(set(var_idx_cat) - set(var_idx_hccv))\n",
    "# components = ['numeric and splines', 'categorical', 'hccv', 'division']\n",
    "df_design_all = pd.concat([df_train_num, train_set[var_idx_cat_not_hccv], df_train_hccv,\n",
    "                           df_train_div, train_set[var_idx_response]], axis=1, sort=False)\n",
    "df_test_all = pd.concat([df_test_num, test_set[var_idx_cat_not_hccv], df_test_hccv,\n",
    "                        df_test_div], axis=1, sort=False)\n",
    "\n",
    "# split design into train/val sets\n",
    "df_train_all, df_val_all = train_test_split(df_design_all, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f1467",
   "metadata": {},
   "source": [
    "Alternative settings: This is used to explore the result. The above cell contains the best alternative from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8007b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Case 1: No transformation\n",
    "# \"\"\"\n",
    "# # exclude hccv from cat variables metadata\n",
    "# var_idx_cat_not_hccv = list(set(var_idx_cat) - set(var_idx_hccv))\n",
    "# # components = ['numeric and splines', 'categorical', 'hccv', 'division']\n",
    "# df_design_all = pd.concat([train_set[var_idx_num], train_set[var_idx_cat_not_hccv], df_train_hccv,\n",
    "#                            train_set[var_idx_response]], axis=1, sort=False)\n",
    "# df_test_all = pd.concat([test_set[var_idx_num], test_set[var_idx_cat_not_hccv], df_test_hccv],\n",
    "#                          axis=1, sort=False)\n",
    "\n",
    "# # split design into train/val sets\n",
    "# df_train_all, df_val_all = train_test_split(df_design_all, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "427db7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Case 2: Add spline\n",
    "# \"\"\"\n",
    "# # exclude hccv from cat variables metadata\n",
    "# var_idx_cat_not_hccv = list(set(var_idx_cat) - set(var_idx_hccv))\n",
    "# # components = ['numeric and splines', 'categorical', 'hccv', 'division']\n",
    "# df_design_all = pd.concat([df_train_num, train_set[var_idx_cat_not_hccv], df_train_hccv,\n",
    "#                            train_set[var_idx_response]], axis=1, sort=False)\n",
    "# df_test_all = pd.concat([df_test_num, test_set[var_idx_cat_not_hccv], df_test_hccv],\n",
    "#                         axis=1, sort=False)\n",
    "\n",
    "# # split design into train/val sets\n",
    "# df_train_all, df_val_all = train_test_split(df_design_all, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe3c533",
   "metadata": {},
   "source": [
    "Case 3, 4, 5 literally use the same setting at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dc9150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Case 3: Add division features\n",
    "# \"\"\"\n",
    "# # exclude hccv from cat variables metadata\n",
    "# var_idx_cat_not_hccv = list(set(var_idx_cat) - set(var_idx_hccv))\n",
    "# # components = ['numeric and splines', 'categorical', 'hccv', 'division']\n",
    "# df_design_all = pd.concat([df_train_num, train_set[var_idx_cat_not_hccv], df_train_hccv,\n",
    "#                            df_train_div, train_set[var_idx_response]], axis=1, sort=False)\n",
    "# df_test_all = pd.concat([df_test_num, test_set[var_idx_cat_not_hccv], df_test_hccv,\n",
    "#                         df_test_div], axis=1, sort=False)\n",
    "\n",
    "# # split design into train/val sets\n",
    "# df_train_all, df_val_all = train_test_split(df_design_all, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cc001",
   "metadata": {},
   "source": [
    "Case 4 was used for our final prediction for Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b7c3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Case 4: Add interaction terms (categorical variables)\n",
    "-- based on variable importances with tree-based selection in Part 2.\n",
    "\"\"\"\n",
    "# exclude hccv from cat variables metadata\n",
    "var_idx_cat_not_hccv = list(set(var_idx_cat) - set(var_idx_hccv))\n",
    "# components = ['numeric and splines', 'categorical', 'hccv', 'division']\n",
    "df_design_all = pd.concat([df_train_num, train_set[var_idx_cat_not_hccv], df_train_hccv,\n",
    "                           df_train_div, train_set[var_idx_response]], axis=1, sort=False)\n",
    "df_test_all = pd.concat([df_test_num, test_set[var_idx_cat_not_hccv], df_test_hccv,\n",
    "                        df_test_div], axis=1, sort=False)\n",
    "\n",
    "# split design into train/val sets\n",
    "df_train_all, df_val_all = train_test_split(df_design_all, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61c4d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Case 5: Add interaction terms (categorical-hccv/numeric)\n",
    "# -- based on normalised coefficient of the previous model above.\n",
    "# \"\"\"\n",
    "# # exclude hccv from cat variables metadata\n",
    "# var_idx_cat_not_hccv = list(set(var_idx_cat) - set(var_idx_hccv))\n",
    "# # components = ['numeric and splines', 'categorical', 'hccv', 'division']\n",
    "# df_design_all = pd.concat([df_train_num, train_set[var_idx_cat_not_hccv], df_train_hccv,\n",
    "#                            df_train_div, train_set[var_idx_response]], axis=1, sort=False)\n",
    "# df_test_all = pd.concat([df_test_num, test_set[var_idx_cat_not_hccv], df_test_hccv,\n",
    "#                         df_test_div], axis=1, sort=False)\n",
    "\n",
    "# # split design into train/val sets\n",
    "# df_train_all, df_val_all = train_test_split(df_design_all, test_size=0.2, random_state=888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fecd1d",
   "metadata": {},
   "source": [
    "Model evaluation log for the five experimental cases:\n",
    "\n",
    "|        |    Explanation    |  Best Alpha   |      Best Lambda      |     Train AUC      |       Val AUC       |\n",
    "|-------:|:------------------|:--------------|:----------------------|:-------------------|---------------------|\n",
    "| Case 1 | No transformation |      1.0      |         7e-05         |      0.84513       |       0.84118       |\n",
    "| Case 2 |    Add spline     |      1.0      |         7e-05         |      0.84631       |       0.84246       |\n",
    "| Case 3 |   Add division    |      1.0      |         7e-05         |      0.84633       |       0.84248       |\n",
    "| Case 4 |  Add interaction  |      0.0      |         0.0010        |      0.85023       |       0.84482       |\n",
    "| Case 5 |  More interaction |      0.0      |         0.0016        |      0.84963       |       0.84448       |\n",
    "\n",
    "Best Model: Case 4 - Spline + Division + Categorical interaction\n",
    "- Alpha: 0.0 -- Ridge regression\n",
    "- Lambda: 0.001\n",
    "\n",
    "From the table, we observed that there is a significant improvement when we incorporate linear spline and adding categorical interaction terms. The improvement of results applies to both the training and the validation sets. \n",
    "\n",
    "Adding division terms does not add much improvement to the results. However, the interaction term with target encoded and numeric variables worsen the performance. This could be that those interaction terms become noise to the model.\n",
    "\n",
    "The first three cases used Lasso regression, while Case 4 and Case 5 used Ridge regression. None of the best models came from Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08b7b6",
   "metadata": {},
   "source": [
    "#### Standardise numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613caa7a",
   "metadata": {},
   "source": [
    "- Standardise train/val set for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a1c1a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-5622a1ff2df5>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_all[num_col] = train_scale\n",
      "/opt/conda/lib/python3.9/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "<ipython-input-40-5622a1ff2df5>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_val_all[num_col] = val_scale\n"
     ]
    }
   ],
   "source": [
    "# get column indices for numerical features that need to standardise\n",
    "# -- index intersection/union are used so we can apply the same function on all the Cases above\n",
    "all_num_col = train_set[var_idx_num].columns.union(var_idx_spline).union(var_idx_div)\n",
    "num_col = df_train_all.columns.intersection(all_num_col)\n",
    "\n",
    "# create standardise pipeline\n",
    "# -- parameters = List of (name, transformer, columns)\n",
    "pipeline = ColumnTransformer([\n",
    "    ('standardise', StandardScaler(), num_col)\n",
    "])\n",
    "\n",
    "# fit standardisation on the train set\n",
    "train_scale = pipeline.fit_transform(df_train_all)\n",
    "# apply the standardisation to the val set\n",
    "val_scale = pipeline.transform(df_val_all)\n",
    "\n",
    "# replace the values with scaled values\n",
    "df_train_all[num_col] = train_scale\n",
    "df_val_all[num_col] = val_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde554bc",
   "metadata": {},
   "source": [
    "- Standardise design/test set for test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4a41207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column indices for numerical features that need to standardise\n",
    "# -- index intersection/union are used so we can apply the same function on all the Cases above\n",
    "all_num_col = train_set[var_idx_num].columns.union(var_idx_spline).union(var_idx_div)\n",
    "num_col = df_design_all.columns.intersection(all_num_col)\n",
    "\n",
    "# create standardise pipeline\n",
    "# -- parameters = List of (name, transformer, columns)\n",
    "pipeline = ColumnTransformer([\n",
    "    ('standardise', StandardScaler(), num_col)\n",
    "])\n",
    "\n",
    "# fit standardisation on the design set\n",
    "# -- need to remove the response during transform so the column index match with the test set\n",
    "design_scale = pipeline.fit_transform(df_design_all.iloc[:,:-1])\n",
    "# apply the standardisation to the test set\n",
    "test_scale = pipeline.transform(df_test_all)\n",
    "\n",
    "# replace the values with scaled values\n",
    "df_design_all[num_col] = design_scale\n",
    "df_test_all[num_col] = test_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792be6f1",
   "metadata": {},
   "source": [
    "#### Start H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c29a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54323 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_292\"; OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~20.04-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)\n",
      "  Starting server from /opt/conda/lib/python3.9/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp5664aubp\n",
      "  JVM stdout: /tmp/tmp5664aubp/h2o_jovyan_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp5664aubp/h2o_jovyan_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54323\n",
      "Connecting to H2O server at http://127.0.0.1:54323 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.32.1.2</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 15 days </td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_jovyan_ib47j5</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>6.971 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54323</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O_API_Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.9.2 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Etc/UTC\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.32.1.2\n",
       "H2O_cluster_version_age:    2 months and 15 days\n",
       "H2O_cluster_name:           H2O_from_python_jovyan_ib47j5\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    6.971 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         accepting new members, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54323\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python_version:             3.9.2 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# connect to h2o via localhost\n",
    "h2o.init(ip=\"localhost\", port=54323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d913b0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# load data to h2o java vm\n",
    "h2o_df_train = h2o.H2OFrame(df_train_all, destination_frame='df_train')\n",
    "h2o_df_val = h2o.H2OFrame(df_val_all, destination_frame='df_val')\n",
    "h2o_df_design = h2o.H2OFrame(df_design_all, destination_frame='df_design')\n",
    "h2o_df_test = h2o.H2OFrame(df_test_all, destination_frame='df_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe75c08",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "- Penalised logistic regression with H2OGeneralizeddLinearEstimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da6626",
   "metadata": {},
   "source": [
    "Use the below cell when training the model without interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f04e3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "                    alpha   model_ids                 auc\n",
      "0                   [1.0]  g1_model_2  0.8457717321312107\n",
      "1    [0.3333333333333333]  g1_model_3   0.845745411029609\n",
      "2    [0.6666666666666666]  g1_model_1  0.8457402048062587\n",
      "3                   [0.0]  g1_model_4  0.8457039653025147\n",
      "\n",
      "Best Alpha: 1.0\n",
      "Best Lambda: 7.068312175009902e-05\n",
      "Train AUC: 0.8463336124884145\n",
      "Val AUC: 0.842472021137556\n",
      "CPU times: user 3.93 s, sys: 316 ms, total: 4.25 s\n",
      "Wall time: 25min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Without Interaction Term\"\"\"\n",
    "# specify dependent/independent variables\n",
    "y = var_idx_response[0]\n",
    "X = list(set(h2o_df_train.columns) - set(var_idx_response))\n",
    "\n",
    "# initialise the tuning grid for alpha\n",
    "# -- the distribution between L1-norm and L2 norm: \n",
    "# -- alpha = 0 corresponds to Ridge Regression\n",
    "# -- alpha = 1 corresponds to Lasso Regression\n",
    "alpha_grids = np.linspace(0, 1, 4).tolist()\n",
    "\n",
    "hyper_parameters = {'alpha': alpha_grids,\n",
    "                   }\n",
    "\n",
    "# setup search criteria\n",
    "criteria = {'strategy': 'RandomDiscrete',\n",
    "            'stopping_metric' : 'auc',\n",
    "            'stopping_rounds': 3,\n",
    "            'seed': 999\n",
    "           }\n",
    "\n",
    "# specify the model\n",
    "grid = H2OGridSearch(H2OGeneralizedLinearEstimator(family='binomial',\n",
    "                                                   nfolds=5,\n",
    "                                                   lambda_search=True,\n",
    "                                                   standardize=False,\n",
    "                                                   seed=999),\n",
    "                     hyper_params=hyper_parameters,\n",
    "                     grid_id='g1',\n",
    "                     search_criteria=criteria)\n",
    "\n",
    "# train the model\n",
    "grid.train(y=y,\n",
    "           x=X,\n",
    "           training_frame=h2o_df_train,\n",
    "           validation_frame=h2o_df_val)\n",
    "\n",
    "# examine the grid performance\n",
    "grid = grid.get_grid(sort_by='auc', decreasing=True)\n",
    "print(grid)\n",
    "\n",
    "# get the best model\n",
    "best_model = grid.models[0]\n",
    "\n",
    "# print results from the best model\n",
    "print(f'Best Alpha: {H2OGeneralizedLinearEstimator.getAlphaBest(best_model)}')\n",
    "print(f'Best Lambda: {H2OGeneralizedLinearEstimator.getLambdaBest(best_model)}')\n",
    "print(f'Train AUC: {best_model.auc(valid=False)}')\n",
    "print(f'Val AUC: {best_model.auc(valid=True)}')\n",
    "\n",
    "# this only show one of the results we tried during the experiment\n",
    "# the result below is corresponding to Case 3.\n",
    "# -- even with seed set, the psuedorandomness in the model training\n",
    "# -- does not give the completely same results concerning very small digits\n",
    "\n",
    "# expected results:\n",
    "# Best Alpha: 1.0\n",
    "# Best Lambda: 7.068312175009902e-05\n",
    "# Train AUC: 0.8463336124884145\n",
    "# Val AUC: 0.842472021137556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81e2d29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f10 1.5837335055382613\n",
      "f03.F 1.1026257955021492\n",
      "f03.E 0.7491052315925488\n",
      "f02 0.383319781381615\n",
      "f09.G 0.33031447739212466\n",
      "f27.F 0.2884689894949824\n",
      "f03.D 0.2832475905124322\n",
      "e11.B 0.21759193806147825\n",
      "e18 0.21407681897055295\n",
      "e14.F 0.17863368581660063\n",
      "f04.A 0.15582334410115833\n",
      "e11.C 0.1485170729105255\n",
      "e14.D 0.12783801719986906\n",
      "f05.D 0.09491096440135381\n",
      "e20.B2D1C 0.0909915310463764\n",
      "f27.B 0.0864175216184668\n",
      "e15_0 0.07830260721736312\n",
      "e03.C 0.07592116543074066\n",
      "e08_3 0.0744209242821456\n",
      "f29.E 0.07132817396952214\n",
      "e20.A5DA8 0.06958239815848948\n",
      "e21.Q 0.06877183915474061\n",
      "f06 0.06767661284354115\n",
      "e24.Q 0.06607980058250987\n",
      "e03.D 0.06234158646287696\n",
      "a02.B 0.06055649739524875\n",
      "e21.C 0.060470302862944164\n",
      "e11.E 0.04187167598784827\n",
      "a03.C 0.04085814140568678\n",
      "c08.B 0.0404900869410792\n",
      "e21.G 0.0400222491752231\n",
      "e21.H 0.03634934664679317\n",
      "a03.L 0.03440098107295733\n",
      "e21.A 0.033275264076709624\n",
      "b02.Y 0.03278288691377278\n",
      "f08 0.032243216418911556\n",
      "e05_3 0.03204430400933947\n",
      "e05_0 0.030211650740889114\n",
      "e13.F 0.029624367694978383\n",
      "a03.H 0.028760170048451182\n",
      "a03.A 0.027420210296643984\n",
      "e20.B805E 0.027053111197389592\n",
      "e21.I 0.026290906270888616\n",
      "f11 0.025907521394603766\n",
      "a02.A 0.02589969232764678\n",
      "f23 0.025177608155312132\n",
      "e02_1 0.023682746960356924\n",
      "e13.D 0.023601385092827956\n",
      "a16.Y 0.022068429855566854\n",
      "f13 0.021361948469060005\n",
      "e20.FBD67 0.01954920123195984\n",
      "e15_2 0.017734943724071264\n",
      "e21.L 0.016637729595271166\n",
      "f01 0.016406935235497605\n",
      "f22 0.015492087347727656\n",
      "e08_0 0.01459037618687866\n",
      "e02_3 0.013563693421416865\n",
      "e13.E 0.012622207845157567\n",
      "c09.A 0.011866982272831266\n",
      "e08_2 0.009039403900078706\n",
      "e02_e15 0.008806561012879608\n",
      "e15_3 0.007819268497490891\n",
      "b04.A 0.004101778316682136\n",
      "e03.G 0.0036638059922837825\n",
      "f09.J 0.0036426060032340862\n",
      "f28 0.0033397550146290604\n",
      "f07.A 0.0029071160512312002\n",
      "f32 0.0028937171811578874\n",
      "f09.F 0.002072196001031835\n",
      "e02_2 0.0020248374909983178\n",
      "e08 0.0010232376439665437\n",
      "e20.C8FEF 0.0006543978985825253\n",
      "e04_e05 0.0005737781706929355\n",
      "f16 0.00015364755602000969\n",
      "e20.025D7 0.0\n",
      "e20.2780B 0.0\n",
      "e20.3D6DA 0.0\n",
      "e20.40897 0.0\n",
      "e20.40EE1 0.0\n",
      "e20.87D4E 0.0\n",
      "e20.A7C3E 0.0\n",
      "e20.A9C76 0.0\n",
      "e20.AF683 0.0\n",
      "e20.BA3CA 0.0\n",
      "e20.D53C5 0.0\n",
      "e20.DCD42 0.0\n",
      "e20.DCF2B 0.0\n",
      "e20.E85F6 0.0\n",
      "e20.F3D59 0.0\n",
      "e20.FFA09 0.0\n",
      "e21.F 0.0\n",
      "e21.M 0.0\n",
      "e21.O 0.0\n",
      "e13.B 0.0\n",
      "e13.C 0.0\n",
      "e13.G 0.0\n",
      "e13.H 0.0\n",
      "e13.J 0.0\n",
      "e13.K 0.0\n",
      "e13.L 0.0\n",
      "e13.M 0.0\n",
      "e13.N 0.0\n",
      "e13.P 0.0\n",
      "e03.A 0.0\n",
      "e03.E 0.0\n",
      "e03.F 0.0\n",
      "e03.H 0.0\n",
      "e03.I 0.0\n",
      "e03.L 0.0\n",
      "e03.N 0.0\n",
      "e03.P 0.0\n",
      "e03.Q 0.0\n",
      "e24.A 0.0\n",
      "e24.B 0.0\n",
      "e24.C 0.0\n",
      "e24.D 0.0\n",
      "e24.E 0.0\n",
      "e24.F 0.0\n",
      "e24.G 0.0\n",
      "e24.H 0.0\n",
      "e24.I 0.0\n",
      "e24.L 0.0\n",
      "e24.N 0.0\n",
      "e24.P 0.0\n",
      "f09.D 0.0\n",
      "f09.E 0.0\n",
      "f09.H 0.0\n",
      "f09.I 0.0\n",
      "f09.K 0.0\n",
      "f09.L 0.0\n",
      "f09.M 0.0\n",
      "a03.E 0.0\n",
      "a03.F 0.0\n",
      "a03.I 0.0\n",
      "a03.J 0.0\n",
      "a03.K 0.0\n",
      "b04.B 0.0\n",
      "b04.C 0.0\n",
      "b04.H 0.0\n",
      "b04.I 0.0\n",
      "a02.C 0.0\n",
      "a02.D 0.0\n",
      "a02.E 0.0\n",
      "a02.F 0.0\n",
      "a02.G 0.0\n",
      "f27.A 0.0\n",
      "f27.C 0.0\n",
      "f27.D 0.0\n",
      "f27.X 0.0\n",
      "e11.F 0.0\n",
      "f29.A 0.0\n",
      "f29.B 0.0\n",
      "f29.C 0.0\n",
      "f29.F 0.0\n",
      "f29.G 0.0\n",
      "f03.G 0.0\n",
      "e14.A 0.0\n",
      "e14.E 0.0\n",
      "e14.Z 0.0\n",
      "a01.B 0.0\n",
      "a01.D 0.0\n",
      "a01.F 0.0\n",
      "a01.G 0.0\n",
      "a01.H 0.0\n",
      "f05.A 0.0\n",
      "f05.B 0.0\n",
      "f05.Z 0.0\n",
      "a13.A 0.0\n",
      "a13.B 0.0\n",
      "a13.E 0.0\n",
      "a13.F 0.0\n",
      "a12.A 0.0\n",
      "a12.B 0.0\n",
      "a12.D 0.0\n",
      "a12.E 0.0\n",
      "f07.B 0.0\n",
      "f07.Z 0.0\n",
      "a16.N 0.0\n",
      "a16.Z 0.0\n",
      "a10.N 0.0\n",
      "a10.Y 0.0\n",
      "c08.C 0.0\n",
      "c06.A 0.0\n",
      "c06.B 0.0\n",
      "c06.C 0.0\n",
      "b07.A 0.0\n",
      "b07.F 0.0\n",
      "c09.C 0.0\n",
      "e01.A 0.0\n",
      "c04.A 0.0\n",
      "c04.B 0.0\n",
      "c04.C 0.0\n",
      "c05.A 0.0\n",
      "c05.B 0.0\n",
      "c05.C 0.0\n",
      "c07.A 0.0\n",
      "c07.C 0.0\n",
      "a17.N 0.0\n",
      "a17.Y 0.0\n",
      "a17.Z 0.0\n",
      "a19.N 0.0\n",
      "a19.Y 0.0\n",
      "a18.E 0.0\n",
      "a18.F 0.0\n",
      "e22.A 0.0\n",
      "e22.B 0.0\n",
      "a20.N 0.0\n",
      "a20.Y 0.0\n",
      "f33.N 0.0\n",
      "f34.N 0.0\n",
      "f34.Y 0.0\n",
      "f30.A 0.0\n",
      "b03.N 0.0\n",
      "b03.Y 0.0\n",
      "e25.A 0.0\n",
      "f25 0.0\n",
      "f15 0.0\n",
      "e09 0.0\n",
      "e23 0.0\n",
      "e05 0.0\n",
      "e05_1 0.0\n",
      "e05_2 0.0\n",
      "e04_2 0.0\n",
      "f02_0 0.0\n",
      "e17 0.0\n",
      "b02.N -4.263193541530738e-06\n",
      "e07 -0.0005171753927611888\n",
      "f31 -0.0006992916933921451\n",
      "e02_0 -0.0008505909739723599\n",
      "c09.B -0.0010038508286270733\n",
      "f17 -0.00124121025528565\n",
      "e08_1 -0.003199822235648617\n",
      "a03.B -0.004575374283884047\n",
      "e21.N -0.00507353893758657\n",
      "b04.E -0.005106842797921775\n",
      "b04.G -0.006002326322418191\n",
      "e21.K -0.006802766334716498\n",
      "f19 -0.007533565486418458\n",
      "a12.C -0.007534621524298177\n",
      "e04_3 -0.00768454665974315\n",
      "b04.D -0.0076925399285996685\n",
      "f24 -0.007828370625621965\n",
      "e04 -0.009930306895514794\n",
      "a13.C -0.010274815006250331\n",
      "e15_1 -0.010869716734849835\n",
      "a03.D -0.011384932264013164\n",
      "c08.A -0.012516348028056744\n",
      "e03.B -0.0126096898730582\n",
      "e04_0 -0.01384659245503325\n",
      "f09.A -0.015021620834332557\n",
      "e25.B -0.019368980206381226\n",
      "f18 -0.020365050632388866\n",
      "a10.Z -0.021907535093837915\n",
      "e06 -0.022890059113474307\n",
      "a02.H -0.023335672148548776\n",
      "Intercept -0.02340882351144824\n",
      "f26 -0.026104580901173062\n",
      "e21.P -0.026242894317176566\n",
      "f02_2 -0.026280294424015256\n",
      "b04.F -0.02792519410916875\n",
      "e11.D -0.028946365935277468\n",
      "e20.54CD2 -0.03198832792093306\n",
      "e04_1 -0.033004239690073184\n",
      "e21.J -0.0331438712894279\n",
      "f09.C -0.03440743235117857\n",
      "e11.G -0.03693843568582711\n",
      "e21.B -0.037520521080922846\n",
      "e20.A4893 -0.04070158991020975\n",
      "f21 -0.04173077618303887\n",
      "f01_f02 -0.04201771543194343\n",
      "e14.B -0.04509582567700286\n",
      "e20.AA9AE -0.05548247353465517\n",
      "f02_3 -0.05694334553492662\n",
      "e13.A -0.059059382813242904\n",
      "f03.B -0.06279139474358834\n",
      "a03.G -0.06308018530642215\n",
      "e21.E -0.06958894485804093\n",
      "e13.I -0.07008533260013408\n",
      "c07.B -0.07110240667874379\n",
      "e15 -0.07125790044401353\n",
      "f33.Y -0.07438184242033535\n",
      "f09.B -0.08382325826222768\n",
      "e01.B -0.08589168746350204\n",
      "e02 -0.0994570280896973\n",
      "e14.C -0.1007864414872802\n",
      "e20.D913A -0.1029837648368199\n",
      "f03.A -0.10586742167293754\n",
      "f04.B -0.11732925365490741\n",
      "f20 -0.13331952989672097\n",
      "e21.D -0.13819209703768007\n",
      "f30.B -0.13923250311489002\n",
      "b07.B -0.165292287150106\n",
      "e20.3424D -0.1694259331971435\n",
      "e01.C -0.17221100965181618\n",
      "f03.C -0.18724393918427126\n",
      "f05.C -0.19667291568950657\n",
      "f27.E -0.2772663481000558\n",
      "a01.A -0.2793751055790001\n",
      "f29.D -0.28474227828626003\n",
      "f02_1 -0.4823334026816091\n",
      "f07.C -0.5521473450090418\n",
      "e11.A -0.5713223743121838\n"
     ]
    }
   ],
   "source": [
    "# print normalised coefficients for each features (sorted descending)\n",
    "# -- this is use when identifying which variables to try for the interaction pairs\n",
    "for key in sorted(best_model.coef_norm(), key=best_model.coef_norm().get, reverse=True):\n",
    "    print(key, best_model.coef_norm()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254175d0",
   "metadata": {},
   "source": [
    "For the case with interaction terms, there seems to be a conflict between cross-validation(`nfolds`) and `interaction_pairs` arguments which causes an error. Also, `H2OGridSearch()` function does not work properly. Therefore, we decided to sacrifice the benefit from cross-validation to appreciate the interaction_pairs built-in functionality from `H2O`. For grid search, we decided to use for loop to perform it manually. \n",
    "\n",
    "Disclaimer: Though we have heard from the email later regarding your suggestion to use `interaction()` function to manually create the interaction terms before feeding into the model training function, we decided to propose our solution to address the issue differently as it, more or less, seems to work fine for this data set (i.e., the model with interaction terms performed best as per our experiment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75b21de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/h2o/estimators/estimator_base.py:200: RuntimeWarning: Dropping bad and constant columns: [e15_4, e02_4, e05_4, e04_4, f02_4, e08_4]\n",
      "  warnings.warn(mesg[\"message\"], RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Alpha: 0.0\n",
      "Best Lambda: 0.0009749633940760714\n",
      "Train AUC: 0.8502375475868515\n",
      "Val AUC: 0.8448112578106212\n",
      "-------------------------------------\n",
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Alpha: 0.3333333333333333\n",
      "Best Lambda: 0.0009395104677450065\n",
      "Train AUC: 0.8477123725660951\n",
      "Val AUC: 0.8437121988532907\n",
      "-------------------------------------\n",
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Alpha: 0.6666666666666666\n",
      "Best Lambda: 0.0009438477190242957\n",
      "Train AUC: 0.8465756363050191\n",
      "Val AUC: 0.8425353122371827\n",
      "-------------------------------------\n",
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Alpha: 1.0\n",
      "Best Lambda: 0.00072346367835892\n",
      "Train AUC: 0.8465686025969433\n",
      "Val AUC: 0.8425245855019254\n",
      "-------------------------------------\n",
      "CPU times: user 3.5 s, sys: 533 ms, total: 4.04 s\n",
      "Wall time: 8min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"With Interaction Terms\"\"\"\n",
    "# specify dependent/independent variables\n",
    "y = var_idx_response[0]\n",
    "X = list(set(h2o_df_train.columns) - set(var_idx_response))\n",
    "\n",
    "# initialise the tuning grid for alpha\n",
    "# -- the distribution between L1-norm and L2 norm: \n",
    "# -- alpha = 0 corresponds to Ridge Regression\n",
    "# -- alpha = 1 corresponds to Lasso Regression\n",
    "alpha_grids = np.linspace(0, 1, 4).tolist()\n",
    "\n",
    "for alpha_ in alpha_grids:\n",
    "    # specify the model\n",
    "    model = H2OGeneralizedLinearEstimator(alpha=alpha_,\n",
    "                                          family='binomial',\n",
    "                                          lambda_search=True,\n",
    "                                          lambda_min_ratio=1e-6,\n",
    "                                          early_stopping=False,\n",
    "                                          standardize=False,\n",
    "                                          interaction_pairs=interaction_pairs,\n",
    "                                          seed=999)\n",
    "\n",
    "    # train the model\n",
    "    model.train(y=y,\n",
    "                x=X,\n",
    "                training_frame=h2o_df_train,\n",
    "                validation_frame=h2o_df_val)\n",
    "    # print the result\n",
    "    print(f'Alpha: {H2OGeneralizedLinearEstimator.getAlphaBest(model)}')\n",
    "    print(f'Best Lambda: {H2OGeneralizedLinearEstimator.getLambdaBest(model)}')\n",
    "    print(f'Train AUC: {model.auc(valid=False)}')\n",
    "    print(f'Val AUC: {model.auc(valid=True)}')\n",
    "    print('-------------------------------------')\n",
    "    \n",
    "# this only show one of the results we tried during the experiment\n",
    "# the result below is corresponding to Case 4 -- the optimal model: Alpha = 0.0 - Ridge regression.\n",
    "# -- even with seed set, the psuedorandomness in the model training\n",
    "# -- does not give the completely same results concerning very small digits\n",
    "\n",
    "# expected results:\n",
    "# Alpha: 0.0\n",
    "# Best Lambda: 0.0009749633940760714\n",
    "# Train AUC: 0.8502312086820765\n",
    "# Val AUC: 0.8448210360918208"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9224cb1",
   "metadata": {},
   "source": [
    "#### Make prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "624d9f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "CPU times: user 54.2 ms, sys: 8.61 ms, total: 62.9 ms\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# specify dependent/independent variables\n",
    "y = var_idx_response[0]\n",
    "X = list(set(h2o_df_design.columns) - set(var_idx_response))\n",
    "\n",
    "# specify the parameter tuned through experiments\n",
    "best_alpha = 0\n",
    "best_lambda = 0.001\n",
    "\n",
    "# train the best model we got with the design set (train+val)\n",
    "model = H2OGeneralizedLinearEstimator(alpha=best_alpha,\n",
    "                                      lambda_=best_lambda,\n",
    "                                      family='binomial',\n",
    "                                      lambda_search=False,\n",
    "                                      standardize=False,\n",
    "                                      seed=999)\n",
    "# train the model\n",
    "model.train(y=y,\n",
    "            x=X,\n",
    "            training_frame=h2o_df_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7e8c5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design AUC: 0.8450130778543783\n"
     ]
    }
   ],
   "source": [
    "# print AUC score on the design set\n",
    "# -- Design AUC: 0.8446103793960728\n",
    "print(f'Design AUC: {model.auc(valid=False)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2f16b",
   "metadata": {},
   "source": [
    "Some classes in the test categorical features are not in the data used for training. H2O automatically deal with these unseen class by \"Skip\" method. \n",
    "\n",
    "(ref: https://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f861538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/h2o/job.py:72: UserWarning: Test/Validation dataset column 'e20' has levels not trained on: [\"30146\", \"BE271\"]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.9/site-packages/h2o/job.py:72: UserWarning: Test/Validation dataset column 'e13' has levels not trained on: [\"Q\", \"S\"]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.9/site-packages/h2o/job.py:72: UserWarning: Test/Validation dataset column 'e03' has levels not trained on: [\"J\", \"Z\"]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.9/site-packages/h2o/job.py:72: UserWarning: Test/Validation dataset column 'e24' has levels not trained on: [\"J\", \"M\", \"Z\"]\n",
      "  warnings.warn(w)\n",
      "/opt/conda/lib/python3.9/site-packages/h2o/job.py:72: UserWarning: Test/Validation dataset column 'a18' has levels not trained on: [\"D\"]\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "# predict the model on design and test sets\n",
    "model_pred_design = model.predict(h2o_df_design[h2o_df_test.columns])\n",
    "model_pred_test   = model.predict(h2o_df_test)\n",
    "\n",
    "# get the probability of the prediction of being '1'\n",
    "model_pred_test = model_pred_test.as_data_frame()['p1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5faf3ca",
   "metadata": {},
   "source": [
    "#### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f6e8b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'unique_id': test_set['unique_id'],\n",
    "    'Predicted': model_pred_test\n",
    "})\n",
    "# save the submission file\n",
    "f_name = 'Group_8_part3.csv'\n",
    "f_path = os.path.join(dirPOutput, f_name)\n",
    "submission.to_csv(f_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f26b09e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.084786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.915669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>0.218722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0.565979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0.099945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296685</th>\n",
       "      <td>2265630</td>\n",
       "      <td>0.918705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296686</th>\n",
       "      <td>2265631</td>\n",
       "      <td>0.097037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296687</th>\n",
       "      <td>2265632</td>\n",
       "      <td>0.590760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296688</th>\n",
       "      <td>2265637</td>\n",
       "      <td>0.178242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296689</th>\n",
       "      <td>2265638</td>\n",
       "      <td>0.143836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296690 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        unique_id  Predicted\n",
       "0               6   0.084786\n",
       "1              16   0.915669\n",
       "2              17   0.218722\n",
       "3              18   0.565979\n",
       "4              19   0.099945\n",
       "...           ...        ...\n",
       "296685    2265630   0.918705\n",
       "296686    2265631   0.097037\n",
       "296687    2265632   0.590760\n",
       "296688    2265637   0.178242\n",
       "296689    2265638   0.143836\n",
       "\n",
       "[296690 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the submission file\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "591128d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_9ffe closed.\n"
     ]
    }
   ],
   "source": [
    "# shutdown h2o cluster\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3aa82",
   "metadata": {},
   "source": [
    "### Note on Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6d9e5",
   "metadata": {},
   "source": [
    "#### Kaggle score: 0.84734\n",
    "<img src=\"../screenshots/Group_8_part3_screenshot.png\">\n",
    "\n",
    "For Part 3, the logical processes for our solution are as follows:\n",
    "- Strategically cleaned the missing values\n",
    "- Applied linear spline (n=5; equidistant percentile between 0,1) to numeric features that have more than 20 unique values\n",
    "- Applied target encoder to the high cardinality categorical variables\n",
    "- Created division terms among numeric variables\n",
    "- Created interaction terms among the most important categorical variables\n",
    "- Created interaction terms between categorical variables and hccv/numeric variables\n",
    "- Tuned the elastic net model with different combinations of features\n",
    "- Fitted the final model on the design set: model = Ridge regression with lambda = 0.001\n",
    "- Predict the test set and create the submission file for the Kaggle competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef372c2",
   "metadata": {},
   "source": [
    "Compare the result to Part 2:\n",
    "\n",
    "| Kaggle Competition   |        Part 2          |        Part 3         |\n",
    "|:---------------------|:-----------------------|:----------------------|\n",
    "|         AUC          |        0.62644         |       0.84734         |\n",
    "\n",
    "From the 5 Cases table above, we noticed that including hccv variables in the training set significantly boosted the model performance, as we can see a large improvement from Part 2. Engineered features such as interaction terms, even though we do not know what each variable represented, can also improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f407478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
